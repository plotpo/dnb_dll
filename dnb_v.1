import requests
from bs4 import BeautifulSoup as soup
import unicodedata
from lxml import etree
import pandas as pd
import csv


#Die Funktion dnb_sru nimmt den Parameter "query" der SRU-Abfrage 
#entgegen und liefert alle Ergebnisse als eine Liste von Records 
#aus. Bei mehr als 100 Records werden weitere Datensätze mit 
#"&startRecord=101" abgerufen (mögliche Werte 1 bis 99.000). 
def dnb_sru(query):
    
    base_url = "https://services.dnb.de/sru/dnb"
    params = {'recordSchema' : 'MARC21-xml',
          'operation': 'searchRetrieve',
          'version': '1.1',
          'maximumRecords': '100',
          'query': query
         }
    r = requests.get(base_url, params=params)
    xml = soup(r.content)
    records = xml.find_all('record', {'type':'Bibliographic'})
    
    if len(records) < 100:
        
        return records
    
    else:
        
        num_results = 100
        i = 101
        while num_results == 100:
            
            params.update({'startRecord': i})
            r = requests.get(base_url, params=params)
            xml = soup(r.content)
            new_records = xml.find_all('record', {'type':'Bibliographic'})
            records+=new_records
            i+=100
            num_results = len(new_records)
            
        return records
    

#Die Funktion parse_records nimmt als Parameter jeweils ein 
#Record entgegen und sucht über xpath die gewünschte 
#Informationen heraus und liefert diese als Dictionary zurück. 
def parse_record(record):
    
    ns = {"marc":"http://www.loc.gov/MARC21/slim"}
    xml = etree.fromstring(unicodedata.normalize("NFC", str(record)))
    
    #year
    year = xml.xpath("marc:datafield[@tag = '264']/marc:subfield[@code = 'c']", namespaces=ns)
    try:
        year = year[0].text
    except:
        year = "unknown"
        
    #author
    author = xml.xpath("marc:datafield[@tag = '100']/marc:subfield[@code = 'a']", namespaces=ns)
    try:
        author = author[0].text
    except:
        author = "unknown"
    
    #titel
    titel = xml.xpath("marc:datafield[@tag = '245']/marc:subfield[@code = 'a']", namespaces=ns)
    try:
        titel = titel[0].text
    except:
        titel = "unknown"
        
    #subtitle
    subtitle = xml.xpath("marc:datafield[@tag = '245']/marc:subfield[@code = 'b']", namespaces=ns)
    try:
        subtitle = subtitle[0].text
    except:
        subtitle = "unknown"          

    #subtitle2
    subtitle2 = xml.xpath("marc:datafield[@tag = '245']/marc:subfield[@code = 'c']", namespaces=ns)
    try:
        subtitle2 = subtitle2[0].text
    except:
        subtitle2 = "unknown"    
        
    #genre ???
    genre = xml.xpath("marc:datafield[@tag = '655']/marc:subfield[@code = 'a']", namespaces=ns)
    try:
        genre = genre[0].text
    except:
        genre = "unknown"     

    #description
    description = xml.xpath("marc:datafield[@tag = '856']/marc:subfield[@code = 'u']", namespaces=ns)
    try:
        description = description[0].text
    except:
        description = "unknown"      
    
    #publisher
    publisher = xml.xpath("marc:datafield[@tag = '264']/marc:subfield[@code = 'b']", namespaces=ns)
    try:
        publisher = publisher[0].text
    except:
        publisher = "unknown"   

    meta_dict = {"year":year, 
                 "author":author,
                 "titel":titel,
                 "subtitle":subtitle,
                 "subtitle2":subtitle2,
                 "genre":genre,
                 "description":description,
                 "publisher":publisher}
    
    return meta_dict

#diese funktion importiert eine liste von namen aus einer .csv-datei
#im gleichen ordner und gibt sie als list aus
#format: vorname nachname,
def import_persons():
    file = open("persons_list.csv", "r", encoding="utf-8")
    persons = list(csv.reader(file, delimiter=","))
    file.close()
    return persons


#Über die verschiedenen Indices 
#https://services.dnb.de/sru/dnb?operation=explain&version=1.1 
#kann die SRU-Abfrage mittels CQL gezielt eingeschrÃ¤nkt werden. 
persons = import_persons()

#Leerer DataFrame wird vor Loop initiiert
df = pd.DataFrame()  

#Loopt durch die Namensliste und sucht jeweils gezielt nach Person,
#dann werden die Ergebnisse dem Dataframe hinzugefügt
for person in persons:
    records = dnb_sru('per=' + str(person))
    print(len(records), 'Ergebnisse')
    output = [parse_record(record) for record in records]
    df = df.append(output, ignore_index=True)

#Dataframe wird als .csv im gleichen Ordner gespeichert
df.to_csv("SRU_Output.csv", index=False)
